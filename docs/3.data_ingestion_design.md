Data Pipeline Design Document
Version: 1.0
Date: June 28, 2025

1. Pipeline Overview
Objective: Ingest and transform energy data from Energy-Charts API into analytics-ready Delta tables.

Key Components:

Bronze Layer: Raw ingested data (3 tables).

Silver Layer: Cleaned, deduplicated data (3 tables).

Gold Layer: Business-ready datasets (3 tables) for BI/ML use cases.

Surrogate Keys: Implemented in Gold tables for immutable unique identifiers.

2. Data Sources & Ingestion
Dataset	Frequency	Bronze Table	Volume/Restrictions
Public Power Data	15-min daily	bronze_Public_power	>1000 rows/month; deduplicate
Price Data	Daily	bronze_Price	<1000 rows/month
Installed Power	Monthly	bronze_Installed_power	<1000 rows/month
Ingestion Configuration:

Adjust frequency, start/end_time, and row_limit in pipeline parameters for backfilling/local testing.
Public Power Data
```python
# Ingest Public Power Data from Energy-Charts API
import requests
from pyspark.sql import SparkSession 
from pyspark.sql.functions import col, current_timestamp
spark = SparkSession.builder.appName("PublicPowerIngestion").getOrCreate()
url = "https://api.energy-charts.info/public_power_data"
response = requests.get(url)
data = response.json()
df = spark.createDataFrame(data)
df = df.withColumn("timestamp", col("timestamp").cast("timestamp")) \
    .withColumn("created_at", current_timestamp()) \
    .withColumn("updated_at", current_timestamp())
df.write.format("delta").mode("overwrite").saveAsTable("bronze_Public_power")
`

3.1 bronze_Public_power → silver_Public_power

# Deduplicate + Merge  
from delta.tables import DeltaTable  

bronze_df = spark.table("bronze_public_power")  
deduped_df = bronze_df.dropDuplicates(["timestamp", "production_type"])  

silver_table = DeltaTable.forName(spark, "silver_Public_power")  
silver_table.alias("target").merge(  
    deduped_df.alias("source"),  
    "target.timestamp = source.timestamp AND target.production_type = source.production_type"  
).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()  

Data Quality Checks:

Null checks on timestamp, production_type, value.

Validate value > 0.

3.2 bronze_Price → silver_Price
python
# Simple merge (low volume)  
bronze_df = spark.table("bronze_Price")  
DeltaTable.merge(  
    bronze_df,  
    "silver_Price",  
    ["timestamp", "production_type"]  
)  
3.3 bronze_Installed_power → silver_Installed_power
python
# Merge with monthly updates  
bronze_df = spark.table("bronze_Installed_power")  
DeltaTable.merge(  
    bronze_df,  
    "silver_Installed_power",  
    ["timestamp", "country"]  
)  
4. Silver to Gold Transformations

4.1 gold_dim_production_type 
```sql
CREATE OR REPLACE TABLE gold_dim_production_type  
USING DELTA  
AS SELECT  
  uuid() AS production_type_id, -- Surrogate key  
  production_type_name,
  CASE 
    WHEN production_type_name IN ('solar', 'wind', 'hydro', 'biomass') THEN 'Renewable'
    WHEN production_type_name IN ('nuclear', 'coal', 'gas', 'oil') THEN 'Non-Renewable'
    ELSE 'Other'
  END AS energy_category,
  CASE 
    WHEN production_type_name IN ('solar', 'wind') THEN 'Weather_Dependent'
    WHEN production_type_name IN ('hydro', 'nuclear', 'coal', 'gas') THEN 'Controllable'
    ELSE 'Mixed'
  END AS controllability_type,
  CASE 
    WHEN production_type_name = 'solar' THEN 'Photovoltaic and solar thermal power'
    WHEN production_type_name = 'wind' THEN 'Onshore and offshore wind power'
    WHEN production_type_name = 'hydro' THEN 'Hydroelectric power generation'
    WHEN production_type_name = 'nuclear' THEN 'Nuclear power generation'
    WHEN production_type_name = 'coal' THEN 'Coal-fired power generation'
    WHEN production_type_name = 'gas' THEN 'Natural gas power generation'
    ELSE production_type_name
  END AS description,
  country,  
  active_flag,  
  current_timestamp() AS created_at,  
  NULL AS updated_at  
FROM (  
  SELECT DISTINCT production_type AS production_type_name, country  
  FROM silver_Public_power  
)
```  
4.2 gold_fact_Power (Enhanced Fact Table with 30-minute Intervals)
```sql
CREATE OR REPLACE TABLE gold_fact_Power  
USING DELTA
PARTITIONED BY (year, month)
AS SELECT  
  year(s.timestamp) AS year,  
  month(s.timestamp) AS month,
  day(s.timestamp) AS day,
  hour(s.timestamp) AS hour,
  minute(s.timestamp) AS minute,
  -- 30-minute intervals for underperformance analysis
  CASE WHEN minute(s.timestamp) < 30 THEN 0 ELSE 30 END AS minute_interval_30,
  date_sub(s.timestamp, INTERVAL minute(s.timestamp) % 30 MINUTE) AS timestamp_30min,
  minute_round(s.timestamp, 5) AS minute_rounded,
  s.timestamp,  
  d.production_type_id,
  d.energy_category,
  d.controllability_type,
  s.value AS electricity_produced,  
  p.price AS electricity_price,  
  s.country,  
  current_timestamp() AS created_at,  
  NULL AS updated_at  
FROM silver_Public_power s  
JOIN silver_Price p ON date_format(s.timestamp, 'yyyy-MM-dd HH') = date_format(p.timestamp, 'yyyy-MM-dd HH')
JOIN gold_dim_production_type d ON s.production_type = d.production_type_name  
WHERE s.value IS NOT NULL AND s.value >= 0
```  

4.3 gold_fact_power_30min_agg (30-minute Aggregated Fact Table for Underperformance Analysis)
```sql
CREATE OR REPLACE TABLE gold_fact_power_30min_agg  
USING DELTA
PARTITIONED BY (year, month)
AS SELECT  
  year(s.timestamp) AS year,  
  month(s.timestamp) AS month,
  day(s.timestamp) AS day,
  hour(s.timestamp) AS hour,
  minute_interval_30,
  timestamp_30min,
  production_type_id,
  energy_category,
  controllability_type,
  
  -- Aggregated metrics for 30-min intervals
  AVG(electricity_produced) AS avg_electricity_produced,
  SUM(electricity_produced) AS total_electricity_produced,
  MIN(electricity_produced) AS min_electricity_produced,
  MAX(electricity_produced) AS max_electricity_produced,
  STDDEV(electricity_produced) AS production_volatility,
  COUNT(*) AS data_points_count,
  
  -- Price metrics
  AVG(electricity_price) AS avg_electricity_price,
  
  country,
  current_timestamp() AS created_at,  
  NULL AS updated_at  
FROM gold_fact_Power  
GROUP BY year, month, day, hour, minute_interval_30, timestamp_30min,
    production_type_id, energy_category, controllability_type, country
```

4.4 gold_performance_baseline (Performance Baseline Table for Underperformance Detection)
```sql
CREATE OR REPLACE TABLE gold_performance_baseline  
USING DELTA  
AS WITH historical_stats AS (
    SELECT 
        production_type_id,
        energy_category,
        controllability_type,
        hour,
        minute_interval_30,
        DAYOFWEEK(timestamp_30min) AS day_of_week,
        CASE 
            WHEN month IN (12, 1, 2) THEN 'Winter'
            WHEN month IN (3, 4, 5) THEN 'Spring'
            WHEN month IN (6, 7, 8) THEN 'Summer'
            ELSE 'Fall'
        END AS season,
        
        -- Statistical baselines
        AVG(avg_electricity_produced) AS baseline_avg_production,
        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY avg_electricity_produced) AS baseline_median_production,
        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY avg_electricity_produced) AS baseline_p25_production,
        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY avg_electricity_produced) AS baseline_p75_production,
        STDDEV(avg_electricity_produced) AS baseline_std_production,
        COUNT(*) AS historical_observations
    FROM gold_fact_power_30min_agg
    WHERE timestamp_30min >= CURRENT_DATE - INTERVAL 90 DAYS
    GROUP BY production_type_id, energy_category, controllability_type,
        hour, minute_interval_30, DAYOFWEEK(timestamp_30min), 
        CASE WHEN month IN (12, 1, 2) THEN 'Winter'
             WHEN month IN (3, 4, 5) THEN 'Spring'
             WHEN month IN (6, 7, 8) THEN 'Summer'
             ELSE 'Fall' END
)
SELECT *,
    -- Define underperformance thresholds
    baseline_avg_production - (baseline_std_production * 1.5) AS underperformance_threshold_moderate,
    baseline_avg_production - (baseline_std_production * 2.0) AS underperformance_threshold_severe,
    baseline_p25_production AS underperformance_threshold_quartile,
    current_timestamp() AS created_at,
    NULL AS updated_at
FROM historical_stats
WHERE historical_observations >= 10
```

5. Enhanced Data Model for BI/ML Use Cases

| Use Case | Source Tables | Key Columns | Description |
|----------|---------------|-------------|-------------|
| **Usecase 1: Public Power Trend Analysis** | `gold_fact_Power`, `gold_dim_production_type` | timestamp, production_type_id, electricity_produced, energy_category | Daily trends and patterns in electricity production by type |
| **Usecase 2: Underperformance Prediction (30min intervals)** | `gold_fact_power_30min_agg`, `gold_performance_baseline` | timestamp_30min, production_type_id, avg_electricity_produced, baseline_avg_production | Predict and detect underperformance patterns with 30-minute granularity |
| **Usecase 3: Price Analysis & Forecasting** | `gold_fact_Power`, `gold_dim_production_type` | timestamp, production_type_id, electricity_price, controllability_type | Price correlation analysis and forecasting models |
| **Usecase 4: Capacity Utilization Monitoring** | `gold_fact_power_30min_agg`, `silver_Installed_power` | production_type_id, total_electricity_produced, installed_capacity | Monitor efficiency and capacity utilization by energy type |

### 5.1 Sample Queries by Use Case

#### Usecase 1: Public Power Trend Analysis
```sql
-- Daily production trends with growth analysis
SELECT  
  DATE(timestamp) AS production_date,
  d.production_type_name,
  d.energy_category,
  SUM(electricity_produced) AS daily_production,
  AVG(electricity_price) AS avg_daily_price,
  LAG(SUM(electricity_produced)) OVER (
    PARTITION BY d.production_type_name 
    ORDER BY DATE(timestamp)
  ) AS previous_day_production
FROM gold_fact_Power f
JOIN gold_dim_production_type d ON f.production_type_id = d.production_type_id
WHERE timestamp >= CURRENT_DATE - INTERVAL 30 DAYS
GROUP BY DATE(timestamp), d.production_type_name, d.energy_category
ORDER BY production_date DESC, daily_production DESC;
```

#### Usecase 2: Underperformance Prediction (30min intervals)
```sql
-- Detect current underperformance and predict next 24 hours
WITH current_performance AS (
  SELECT 
    f.timestamp_30min,
    d.production_type_name,
    d.controllability_type,
    f.avg_electricity_produced AS actual_production,
    b.baseline_avg_production AS expected_production,
    CASE 
      WHEN f.avg_electricity_produced < b.underperformance_threshold_severe THEN 'Severe'
      WHEN f.avg_electricity_produced < b.underperformance_threshold_moderate THEN 'Moderate'
      ELSE 'Normal'
    END AS performance_status,
    ((f.avg_electricity_produced - b.baseline_avg_production) / b.baseline_avg_production) * 100 AS deviation_pct
  FROM gold_fact_power_30min_agg f
  JOIN gold_dim_production_type d ON f.production_type_id = d.production_type_id
  JOIN gold_performance_baseline b ON (
    f.production_type_id = b.production_type_id 
    AND f.hour = b.hour 
    AND f.minute_interval_30 = b.minute_interval_30
  )
  WHERE f.timestamp_30min >= CURRENT_TIMESTAMP - INTERVAL 4 HOUR
)
SELECT * FROM current_performance 
WHERE performance_status IN ('Severe', 'Moderate')
ORDER BY timestamp_30min DESC, deviation_pct ASC;
```  

6. Monitoring & Data Quality
Framework: PySpark + Great Expectations

Checks:

Row counts post-ingestion.

Null values in key columns (timestamp, production_type_id).

Negative values in electricity_produced.

Alerting: Log failed checks to AWS CloudWatch / Azure Monitor.

Components:

Storage: Azure Data Lake (Delta format).

Orchestration: Azure Data Factory (pipeline triggers).

Processing: Databricks (PySpark).

Monitoring: CloudWatch + Great Expectations.

8. Key Decisions
Surrogate Keys:

UUID() for production_type_id ensures uniqueness across environments.

Avoids dependency on source-system keys.

Delta Merge Operations:

Optimized for deduplication and incremental updates.

Local Testing:

Limit data volume with ingestion parameters and proceesing maximum 1 day data ata time.

Scalability:

Partition Gold tables by timestamp for query optimization.